Bora resolver isso de vez. Eu li seu AssistantBar.tsx e o laudo que vocÃª mandou. O backend estÃ¡ OK; o problema estÃ¡ no frontend (corrida entre streams + um fetchSuggest que sobrescreve os cards recebidos por SSE). Vou te dar patches cirÃºrgicos que param os sumiÃ§os dos produtos, evitam frases repetidas e consertam o â€œdigitandoâ€.

3 bugs que causam exatamente o que vocÃª descreveu (com base no seu arquivo):

Sem controle de â€œrequisiÃ§Ã£o atualâ€: vocÃª nÃ£o usa requestId (nem descarta chunks antigos). Quando outro stream comeÃ§a, os eventos do anterior ainda chegam e bagunÃ§am estado.

fetchSuggest() roda depois do SSE e limpa topBox/feed com um resultado diferente (Ã s vezes vazio). Resultado: os produtos aparecem e somem logo depois.

Typing: vocÃª liga setIsTyping(true), mas desliga antes do primeiro delta (logo apÃ³s fetch()), entÃ£o o â€œdigitandoâ€ some e o usuÃ¡rio acha que travou. 

AssistantBar_ANALISE

A seguir, patches prontos. Cole exatamente onde indicado.

1) Trave a sessÃ£o do stream por requestId (descartar eventos velhos)

Acima dos estados, adicione:

const activeRequestIdRef = useRef<string | null>(null);
const haveProductsInThisRequestRef = useRef(false); // trava contra fetchSuggest sobrescrever


Dentro de startStream, logo apÃ³s fetch retornar (e antes de ler o body), gere e salve um id:

// gere um requestId local (se o backend nÃ£o enviar)
const reqId = `r-${Date.now()}-${Math.random().toString(36).slice(2,8)}`;
activeRequestIdRef.current = reqId;
haveProductsInThisRequestRef.current = false;


Crie um helper pra aceitar apenas eventos da requisiÃ§Ã£o corrente:

const acceptEvent = (payload: any) => {
  // Se o backend mandar requestId no JSON, valide:
  if (payload?.requestId && activeRequestIdRef.current && payload.requestId !== activeRequestIdRef.current) {
    console.log('â­ï¸ descartando evento de request antigo', payload.requestId);
    return false;
  }
  return true;
};


Dentro do loop de chunks, antes de processar cada p (JSON parseado), valide:

if (!acceptEvent(p)) continue;


Resultado: um novo startStream invalida o anterior; nada â€œapagaâ€ a resposta em curso. 

AssistantBar_ANALISE

2) Pare o fetchSuggest de sobrescrever os produtos do SSE
2.1. SÃ³ permita fetchSuggest se ainda nÃ£o chegaram produtos neste request

No bloco onde vocÃª chama fetchSuggest quando detecta palavras no texto do assistente, troque:

if (pendingSearchRef.current && !hasTriggeredSearchRef.current && (
  assistantMessage.toLowerCase().includes('busca') ||
  assistantMessage.toLowerCase().includes('procurando') ||
  assistantMessage.toLowerCase().includes('opÃ§Ãµes') ||
  assistantMessage.toLowerCase().includes('aqui estÃ£o') ||
  assistantMessage.toLowerCase().includes('vou buscar') ||
  assistantMessage.toLowerCase().includes('procurar')
)) {
  fetchSuggest(pendingSearchRef.current);
  hasTriggeredSearchRef.current = true;
  pendingSearchRef.current = '';
}


por:

const spokeAboutSearch = /busca|procurando|opÃ§Ãµes|aqui estÃ£o|vou buscar|procurar/i.test(assistantMessage);
if (pendingSearchRef.current && !hasTriggeredSearchRef.current && spokeAboutSearch && !haveProductsInThisRequestRef.current) {
  // SÃ³ usamos suggest como fallback ANTES de chegar produtos reais
  fetchSuggest(pendingSearchRef.current);
  hasTriggeredSearchRef.current = true;
  // NÃƒO limpe pendingSearch ainda â€” deixe o SSE "products" decidir
}

2.2. Marque que chegaram produtos pela IA e impeÃ§a que o suggest limpe

No handler de p.type === 'products', antes de setTopBox(...):

haveProductsInThisRequestRef.current = true;


E dentro de fetchSuggest, nÃ£o limpe produtos se jÃ¡ temos produtos neste request:

const fetchSuggest = async (q: string) => {
  if (!q.trim() || loadingSug) return;
  setLoadingSug(true);

  try {
    // Se jÃ¡ chegaram produtos por SSE neste request, NÃƒO toque na UI de produtos
    if (haveProductsInThisRequestRef.current) {
      console.log('â†©ï¸ fetchSuggest cancelado: jÃ¡ hÃ¡ produtos do SSE neste request');
      return;
    }

    // ... restante do cÃ³digo como estÃ¡ ...
  } finally {
    setLoadingSug(false);
  }
};


Agora os produtos nÃ£o somem: o suggest vira fallback (antes do SSE) e nÃ£o um â€œlimpadorâ€ AFTER. 

AssistantBar_ANALISE

3) Conserte o â€œdigitandoâ€ (sÃ³ desligar apÃ³s o 1Âº delta)

No inÃ­cio do startStream remova este trecho:

// Remover indicador de digitaÃ§Ã£o quando comeÃ§ar a receber resposta
setIsTyping(false);


E ligue/desligue assim:

setIsTyping(true);   // antes de iniciar a leitura

// dentro do loop: quando receber o primeiro delta/chunk vÃ¡lido
if ((p.type === 'chunk' || p.type === 'delta') && p.text) {
  if (isTyping) setIsTyping(false); // desligar aqui, no primeiro delta real
  // ... resto inalterado ...
}


Com isso, o â€œdigitandoâ€ fica visÃ­vel atÃ© o primeiro texto chegar, evitando a sensaÃ§Ã£o de sumiÃ§o. 

AssistantBar_ANALISE

4) NÃ£o fechar o stream â€œmudoâ€ â†’ sempre promova o texto/mostre fallback

No fim do while (true) (quando done), vocÃª jÃ¡ tem um fallback. Garanta que tambÃ©m adiciona a Ãºltima mensagem ao chat mesmo se o backend mandar complete sem deltas:

// jÃ¡ existe um fallback depois do loop â€” mantenha.
// mas dentro do handler de 'end'/'complete', retire o 'return' seco:
} else if (p.type === 'end' || p.type === 'complete') {
  // ... seu cÃ³digo ...
  if (assistantMessage.trim()) {
    setChatMessages(prev => [...prev, { type: 'assistant', text: assistantMessage.trim() }]);
  } else {
    // fallback verbal se nÃ£o veio nada:
    setChatMessages(prev => [...prev, { type: 'assistant', text: "Estou aqui ğŸ‘ Me diga a categoria e a cidade que eu jÃ¡ busco ofertas." }]);
  }
  setStreaming('');
  // NÃƒO dÃª 'return' â€” apenas marque leitura como encerrada e deixe o loop quebrar naturalmente
}


Evita â€œacaba o stream e nÃ£o aparece nadaâ€. 

AssistantBar_ANALISE

5) Dupla compatibilidade de eventos (se o backend alternar formato)

Seu parser hoje sÃ³ lida com data: {...} (JSON). Se o backend enviar eventos nomeados (event: products), o seu .replace(/^data:/,'') vai deixar â€œevent: ...\ndata: ...â€ e cair no â€œJSON malformadoâ€.

Patch rÃ¡pido no parsing do part:

// antes de JSON.parse(line):
let dataPayload = line;
if (line.startsWith('event:')) {
  // formato SSE nomeado: 'event: X\ndata: {...}'
  const lines = line.split('\n');
  const dataLine = lines.find(l => l.startsWith('data:'));
  dataPayload = dataLine ? dataLine.replace(/^data:\s?/, '') : '';
}
const p = JSON.parse(dataPayload);


Assim, vocÃª aguenta tanto onmessage {type:...} quanto event: nomeado â€” sem quebrar. 

AssistantBar_ANALISE

6) Evite duplicar submissÃµes (frases repetidas)

VocÃª jÃ¡ tem throttling no header, mas o formulÃ¡rio tambÃ©m chama startStream. Garanta que um caminho dispara por vez:

No listener handleHeaderSubmit, retorne false/stopPropagation no evento customizado (se vocÃª controlar o emissor), ou adicione uma flag curta:

const firingRef = useRef(false);

// dentro de handleHeaderSubmit, logo no inÃ­cio:
if (firingRef.current) return;
firingRef.current = true;
setTimeout(() => (firingRef.current = false), 800);


Isso elimina as mensagens repetidas por turnos duplicados. 

AssistantBar_ANALISE

Checklist rÃ¡pido (valide aÃ­)

 Produtos nÃ£o somem mais (SSE â€œganhaâ€ do suggest; suggest nÃ£o limpa).

 â€œDigitandoâ€ sÃ³ apaga apÃ³s o 1Âº delta.

 Nada de frases repetidas (anti-duplo disparo).

 Eventos antigos sÃ£o ignorados (requestId/guard).

 Parser lida com eventos nomeados e {type:...}.